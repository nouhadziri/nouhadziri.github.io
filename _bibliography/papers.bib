---
---

@string{aps = {American Physical Society,}}




@inproceedings{Kaleidoscope,
  title={Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties},
  author={Taylor Sorensen  and Liwei Jiang  and Jena Hwang and Sydney Levine  and Valentina Pyatkin  and Peter West and Nouha Dziri  and Ximing Lu  and Kavel Rao  and Chandra Bhagavatula  and Maarten Sap  and John Tasioulas  and Yejin Choi},
  booktitle={arXiv},
  month=Sept,
  year={2023},
  url={https://arxiv.org/pdf/2309.00779},
  preview={kaleido.png},
  arxiv={2309.00779},
  code={https://github.com/tsor13/kaleido},
  website={https://kaleido.allen.ai/},
  data={https://huggingface.co/datasets/allenai/ValuePrism},
abstract={Visual information is central to conversation: body gestures and facial expressions, for example, contribute to meaning that transcends words alone. To date, however, most neural conversational models are limited to just text. We introduce CHAMPAGNE, a generative model of conversations that can account for visual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that converts error-prone automatic transcripts to a cleaner dialogue format while maintaining meaning. Human evaluation reveals that YTD-18M is more sensible and specific than prior resources (MMDialog, 1M dialogues), while maintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE learns to conduct conversation from YTD-18M; and 2) when fine-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations. We release data, models, and code at https://seungjuhan.me/champagne.}
}

@inproceedings{faith,
  title={Faith and Fate: Limits of Transformers on Compositionality},
  author={Nouha {Dziri} and
                  Ximing {Lu} and
                  Melanie {Sclar} and
                  Xiang Lorraine {Li} and
                  Liwei {Jiang} and
                  Bill Yuchen {Lin} and
                  Peter {West} and
                  Chandra {Bhagavatula} and
                  Ronan {Le Bras} and
                  Jena D. {Hwang} and
                  Soumya {Sanyal} and
                  Sean {Welleck} and
                  Xiang {Ren} and
                  Allyson {Ettinger} and
                  Zaid {Harchaoui} and
                  Yejin {Choi}},
  booktitle={Neurips},
  month=June,
  year={2023},
  url={https://arxiv.org/abs/2305.18654},
  preview={faith.png},
  arxiv={2305.18654},
  abstract={Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how Transformers' performance will rapidly decay with increased task complexity.}
  }




@inproceedings{rlhf,
  title={Fine-Grained Human Feedback Gives Better Rewards for Language Model Training},
  author={Zeqiu Wu  and Yushi Hu  and Weijia Shi  and Nouha Dziri  and Alane Suhr  and Prithviraj Ammanabrolu  and Noah Smith  and Mari Ostendorf  and Hannaneh Hajishirzi},
  booktitle={Neurips},
  month=June,
  year={2023},
  url={https://arxiv.org/pdf/2306.01693},
  preview={rlhf.png},
  arxiv={2306.01693},
  code={https://github.com/allenai/FineGrainedRLHF},
    website={https://finegrainedrlhf.github.io/},
abstract={Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.}
}

@inproceedings{ethic,
  title={What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations},
  author={Kavel Rao, Liwei Jiang, Valentina Pyatkin, Yuling Gu, Faeze Brahman, Niket Tandon, Nouha Dziri, Faeze Brahman, Yejin Choi},
  booktitle={EMNLP},
  month=May,
  year={2023},
  url={},
  preview={defeasible.png},
  arxiv={},
  abstract={}
}

@inproceedings{ipa,
  title={Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning},
  author={Ximing Lu and Faeze Brahman and Peter West and Jaehun Jang and Khyathi Chandu and Abhilasha Ravichander and Lianhui Qin and Prithviraj Ammanabrolu and Liwei Jiang and Sahana Ramnath and Nouha Dziri and Jillian Fisher and Bill Yuchen Lin and Skyler Hallinan and Xiang Ren and Sean Welleck and Yejin Choi},
  booktitle={EMNLP},
  month=May,
  year={2023},
  url={https://arxiv.org/pdf/2305.15065},
  preview={ipa.png},
  arxiv={2305.15065},
abstract={Large language models excel at a variety of language tasks when prompted with examples or instructions. Yet controlling these models through prompting alone is limited. Tailoring language models through fine-tuning (e.g., via reinforcement learning) can be effective, but it is expensive and requires model access. We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adaptor trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and open-domain generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT- 3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.}
}

@inproceedings{refine,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Aman Madaan and
                  Niket Tandon and
                  Prakhar Gupta and
                  Skyler Hallinan and
                  Luyu Gao and
                  Sarah Wiegreffe and
                  Uri Alon and
                  Nouha Dziri and
                  Shrimai Prabhumoye and
                  Yiming Yang and
                  Sean Welleck and
                  Bodhisattwa Prasad Majumder and
                  Shashank Gupta and
                  Amir Yazdanbakhsh and
                  Peter Clark},
  booktitle={Neurips},
  month=Mar,
  year={2023},
  url={https://arxiv.org/abs/2303.17651},
  arxiv={2303.17651},
  preview={refine.png},
  website={https://selfrefine.info/},
  code={https://github.com/madaan/self-refine},
abstract={Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.}}

@inproceedings{CHAMPAGNE,
  title={CHAMPAGNE: Learning Real-world Conversation
from Large-Scale Web Videos},
  author={Seungju Han  and Jack Hessel  and Nouha Dziri  and Yejin Choi  and Youngjae Yu},
  booktitle={ICCV},
  month=March,
  year={2023},
  url={https://arxiv.org/pdf/2303.09713},
  website={https://seungjuhan.me/champagne/},
  code={https://github.com/wade3han/champagne},
  preview={champagne.png},
  arxiv={2303.09713},
abstract={Visual information is central to conversation: body gestures and facial expressions, for example, contribute to meaning that transcends words alone. To date, however, most neural conversational models are limited to just text. We introduce CHAMPAGNE, a generative model of conversations that can account for visual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that converts error-prone automatic transcripts to a cleaner dialogue format while maintaining meaning. Human evaluation reveals that YTD-18M is more sensible and specific than prior resources (MMDialog, 1M dialogues), while maintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE learns to conduct conversation from YTD-18M; and 2) when fine-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations. We release data, models, and code at https://seungjuhan.me/champagne.}
}


@inproceedings{elastic,
  title={Elastic weight removal for faithful and abstractive dialogue generation},
  author={Nico Daheim  and Nouha Dziri  and Mrinmaya Sachan  and Iryna Gurevych  and Edoardo M Ponti},
  month=March,
  year={2023},
  booktitle={arXiv},
  journal={ArXiv},
  code={https://github.com/ndaheim/faithful-dialogue},
  arxiv={2303.17574},
  preview={ewr.png},
abstract={Ideally, dialogue systems should generate responses that are faithful to the knowledge contained in relevant documents. However, many models generate hallucinated responses instead that contradict it or contain unverifiable information. To mitigate such undesirable behaviour, it has been proposed to fine-tune a `negative expert' on negative examples and subtract its parameters from those of a pre-trained model. However, intuitively, this does not take into account that some parameters are more responsible than others in causing hallucinations. Thus, we propose to weigh their individual importance via (an approximation of) the Fisher Information matrix, which measures the uncertainty of their estimate. We call this method Elastic Weight Removal (EWR). We evaluate our method -- using different variants of Flan-T5 as a backbone language model -- on multiple datasets for information-seeking dialogue generation and compare our method with state-of-the-art techniques for faithfulness, such as CTRL, Quark, DExperts, and Noisy Channel reranking. Extensive automatic and human evaluation shows that EWR systematically increases faithfulness at minor costs in terms of other metrics. However, we notice that only discouraging hallucinations may increase extractiveness, i.e. shallow copy-pasting of document spans, which can be undesirable. Hence, as a second main contribution, we show that our method can be extended to simultaneously discourage hallucinations and extractive responses. We publicly release the code for reproducing EWR and all baselines.}}


@inproceedings{evalOpenQA,
  title={Evaluating Open-Domain Question Answering in the Era of Large Language Models},
  author={{Kamalloo}, Ehsan and {Dziri}, Nouha and {Clarke}, Charles and {Rafiei}, Davood},
  month = jul,
  year = {2023},
  code={https://github.com/ehsk/OpenQA-eval},
  url={https://aclanthology.org/2023.acl-long.307/},
  arxiv={2305.06984},
  preview={qa.png},
  booktitle = {ACL (oral)},
  location = {Toronto, Canada},
  abstract={Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-open. We also find that more than 50% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.},
}



@article{begin,
  title={Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark},
  author={Nouha Dziri and Hannah Rashkin and Tal Linzen and David Reitter},
  journal={TACL},
  month=May,
  year={2022},
  pdf={https://aclanthology.org/2022.tacl-1.62.pdf},
  code={https://github.com/google/BEGIN-dataset},
  preview={begin.png},
  abstract={The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. Dziri et al. (2022)'s investigation of hallucinations has revealed that existing knowledge-grounded benchmarks are contaminated with hallucinated responses at an alarming level (>60% of the responses) and models trained on this data amplify hallucinations even further (>80% of the responses). To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as a training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 21.1 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging.},
}


@article{faithdial,
  title={FaithDial: A Faithful Benchmark for Information-Seeking Dialogue},
  author={{Dziri}, Nouha and {Kamalloo}, Ehsan and {Milton}, Sivan and Zaiane, Osmar and Yu, Mo and Ponti, Edoardo and Reddy, Siva},
  journal={TACL},
  month=apr,
  year={2022},
  url={https://arxiv.org/abs/2204.10757},
  arxiv={2204.10757},
  website={https://mcgill-nlp.github.io/FaithDial/},
  code={https://github.com/McGill-NLP/FaithDial},
  preview={faithdial.png},
  abstract={The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. Dziri et al. (2022)'s investigation of hallucinations has revealed that existing knowledge-grounded benchmarks are contaminated with hallucinated responses at an alarming level (>60% of the responses) and models trained on this data amplify hallucinations even further (>80% of the responses). To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as a training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 21.1 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging.},
}


@article{hall,
  title={On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?},
  author={Nouha Dziri and Sivan Milton and Mo Yu and Osmar Zaiane and Siva Reddy},
  journal={NAACL},
  month=Jul,
  year={2022},
  url={https://arxiv.org/pdf/2204.07931},
  arxiv={2204.07931},
  code={https://github.com/McGill-NLP/FaithDial},
  preview={hall.png},
abstract={Knowledge-grounded conversational models are known to suffer from producing factually invalid statements, a phenomenon commonly called hallucination. In this work, we investigate the underlying causes of this phenomenon: is hallucination due to the training data, or to the models? We conduct a comprehensive human study on both existing knowledge-grounded conversational benchmarks and several state-of-the-art models. Our study reveals that the standard benchmarks consist of >60% hallucinated responses, leading to models that not only hallucinate but even amplify hallucinations. Our findings raise important questions on the quality of existing datasets and models trained using them. We make our annotations publicly available for future research.}
}


@article{nph,
  title={Neural path hunter: Reducing hallucination in dialogue systems via path grounding},
  author={Nouha Dziri and Andrea Madotto and Osmar Zaiane and Avishek Joey Bose},
  journal={EMNLP},
  month=Nov,
  year={2021},
  url={https://arxiv.org/pdf/2104.08455},
  arxiv={2104.08455},
  code={https://github.com/nouhadziri/Neural-Path-Hunter},
  preview={nph.png},
abstract={Dialogue systems powered by large pre-trained language models (LM) exhibit an innate ability to deliver fluent and natural-looking responses. Despite their impressive generation performance, these models can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving the faithfulness -- and thus reduce hallucination -- of Neural Dialogue Systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the k-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage consisting of a chain of two neural LM's that retrieves correct entities by crafting a query signal that is propagated over the k-hop subgraph. Our proposed model can easily be applied to any dialogue generated responses without retraining the model. We empirically validate our proposed approach on the OpenDialKG dataset against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020).}}

@article{demi,
  title={Decomposed mutual information estimation for contrastive representation learning},
  author={Alessandro Sordoni* and Nouha Dziri* and Hannes Schulz* and Geoff Gordon and Philip Bachman and Remi Tachet Des Combes},
  journal={ICML},
  month=Jul,
  year={2021},
  pdf={http://proceedings.mlr.press/v139/sordoni21a/sordoni21a.pdf},
  preview={demi.png},
abstract={Recent contrastive representation learning methods rely on estimating mutual information (MI) between multiple views of an underlying context. Eg, we can derive multiple views of a given image by applying data augmentation, or we can split a sequence into views comprising the past and future of some step in the sequence. Contrastive lower bounds on MI are easy to optimize, but have a strong underestimation bias when estimating large amounts of MI. We propose decomposing the full MI estimation problem into a sum of smaller estimation problems by splitting one of the views into progressively more informed subviews and by applying the chain rule on MI between the decomposed views. This expression contains a sum of unconditional and conditional MI terms, each measuring modest chunks of the total MI, which facilitates approximation via contrastive bounds. To maximize the sum, we formulate a contrastive lower bound on the conditional MI which can be approximated efficiently. We refer to our general approach as Decomposed Estimation of Mutual Information (DEMI). We show that DEMI can capture a larger amount of MI than standard non-decomposed contrastive bounds in a synthetic setting, and learns better representations in a vision domain and for dialogue generation.}
}




@inproceedings{dziri-etal-2019-evaluating,
    title = "Evaluating Coherence in Dialogue Systems using Entailment",
    author = "Dziri, Nouha  and
      Kamalloo, Ehsan  and
      Mathewson, Kory  and
      Zaiane, Osmar",
    booktitle = "NAACL-HLT",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    url = "https://aclanthology.org/N19-1381",
    doi = "10.18653/v1/N19-1381",
    pages = "3806--3812",
    preview={eval.png},
    abstract = "Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.",
    code={https://github.com/nouhadziri/DialogEntailment},
}


@inproceedings{THRED,
    title = "Augmenting Neural Response Generation with Context-Aware Topical Attention",
    author = "Dziri, Nouha  and
      Kamalloo, Ehsan  and
      Mathewson, Kory  and
      Zaiane, Osmar",
    booktitle = "Proceedings of the First Workshop on NLP for Conversational AI (NLP4ConvAI) at ACL 2019",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4103",
    doi = "10.18653/v1/W19-4103",
    pages = "18--31",
    preview={thred.png},
    abstract = "Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges. Notwithstanding the syntactically well-formed responses generated by these neural network models, they are prone to be acontextual, short and generic. In this work, we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven, multi-turn response generation system intended to produce contextual and topic-aware responses. Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation. To train our model, we provide a clean and high-quality conversational dataset mined from Reddit comments. We evaluate THRED on two novel automated metrics, dubbed Semantic Similarity and Response Echo Index, as well as with human evaluation. Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines.",
    code={https://github.com/nouhadziri/THRED},
    preview={THRED.png},

}




@inproceedings{huang-etal-2018-automatic,
    title = "Automatic Dialogue Generation with Expressed Emotions",
    author = {Huang, Chenyang  and
      Za{\"\i}ane, Osmar  and
      Trabelsi, Amine  and
      Dziri, Nouha},
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2008",
    code ={https://github.com/chenyangh/DialogueGenerationWithEmotion},
    doi = "10.18653/v1/N18-2008",
    preview={emotion.png},
    pages = "49--54",
    abstract = "Despite myriad efforts in the literature designing neural dialogue generation systems in recent years, very few consider putting restrictions on the response itself. They learn from collections of past responses and generate one based on a given utterance without considering, speech act, desired style or emotion to be expressed. In this research, we address the problem of forcing the dialogue generation to express emotion. We present three models that either concatenate the desired emotion with the source input during the learning, or push the emotion in the decoder. The results, evaluated with an emotion tagger, are encouraging with all three models, but present better outcome and promise with our model that adds the emotion vector in the decoder.",
}


