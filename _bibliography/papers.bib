---
---
@string{aps = {American Physical Society,}}

@inproceedings{hivemind,
  title={ Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)},
  author={Liwei Jiang and Chai Yuanjun and Margaret Li and Mickel Liu and Raymond Fok and Maarten Sap and Yulia Tsvetkov and Nouha Dziri and Yejin Choi},
  booktitle={NeurIPS (Oral)},
  month={},
  year={2025},
  preview={hivemind.png},
  abstract={Language models (LMs) often struggle to generate diverse, human-like creative content, raising concerns about the long-term homogenization of human thought through repeated exposure to similar outputs. Yet, scalable methods for evaluating LM output diversity remain limited—especially beyond narrow tasks like random number generation or stylized prompts. To address this gap, we introduce InfiniteChats, a large-scale dataset of 26,000 diverse, real-world open-ended user queries, along with the first comprehensive taxonomy for characterizing the full spectrum of open-ended prompts posed to LMs, comprising six top-level categories and 17 subcategories. These queries admit a wide range of plausible answers with no single ground truth. Using InfiniteChats, we present a large-scale analysis of mode collapse in LMs, manifested as redundant outputs even for inherently open-ended queries. Our study reveals a pronounced "Artificial Hivemind" effect in open-ended generation, characterized by (1) intra-model repetition, where a single model consistently generates similar responses, and (2) inter-model homogeneity, where different models produce strikingly similar outputs.InfiniteChats also includes 31,250 human annotations, across absolute ratings and pairwise preferences, with 25 independent human annotations per example. This enables fine-grained analysis of distributional preferences across annotators. Our findings show that state-of-the-art LMs, reward models, and LM judges align less with human ratings when annotators disagree or when responses are of similar quality. Overall, InfiniteChats offers the first large-scale resource for systematically studying open-endedness in LM queries, revealing critical insights to guide future research and mitigate long-term AI safety risks posed by the Artificial Hivemind.}
}


@inproceedings{delta,
  title={RL Grokking Recipe: How Does RL Unlock and Transfer New Algorithms in LLMs?},
  author={Yiyou Sun and Shawn Hu and Georgia Zhou and Ken Zheng and Hannaneh Hajishirzi and Nouha Dziri* and Dawn Song*},
  booktitle={Arxiv},
  month=September,
  year={2025},
  arxiv={2509.21016},
  url={https://arxiv.org/abs/2509.21016},
  preview={delta.png},
  code={https://github.com/sunblaze-ucb/rl-grok-recipe},
  abstract={It remains an open question whether LLMs can acquire or generalize genuinely new reasoning strategies, beyond the sharpened skills encoded in their parameters during pre-training or post-training. To attempt to answer this debate, we introduce DELTA-Code -- Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding -- a controlled benchmark of synthetic coding problem families designed to probe two fundamental aspects: learnability -- can LLMs, through reinforcement learning (RL), solve problem families where pretrained models exhibit failure with large enough attempts (pass@K=0)? -- and transferrability -- if learnability happens, can such skills transfer systematically to out-of-distribution (OOD) test sets? Unlike prior public coding datasets, DELTA isolates reasoning skills through templated problem generators and introduces fully OOD problem families that demand novel strategies rather than tool invocation or memorized patterns. Our experiments reveal a striking grokking phase transition: after an extended period with near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To enable learnability on previously unsolvable problem families, we explore key training ingredients such as staged warm-up with dense rewards, experience replay, curriculum training, and verification-in-the-loop. Beyond learnability, we use DELTA to evaluate transferability or generalization along exploratory, compositional, and transformative axes, as well as cross-family transfer. Results show solid gains within families and for recomposed skills, but persistent weaknesses in transformative cases. DELTA thus offers a clean testbed for probing the limits of RL-driven reasoning and for understanding how models can move beyond existing priors to acquire new algorithmic skills.}
}

@inproceedings{openagentsafety,
title={OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety},
author={Sanidhya Vijayvargiya and Aditya Bharat Soni and Xuhui Zhou and Zora Zhiruo Wang and Nouha Dziri and Graham Neubig and Maarten Sap},
booktitle={Arxiv},
month={July},
year={2025},
arxiv={2507.06134},
url={https://arxiv.org/abs/2507.06134},
preview={openagentsafety.png},
code={https://github.com/sani903/OpenAgentSafety},
abstract={Recent advances in AI agents capable of solving complex, everyday tasks, from scheduling to customer service, have enabled deployment in real-world settings, but their possibilities for unsafe behavior demands rigorous evaluation. While prior benchmarks have attempted to assess agent safety, most fall short by relying on simulated environments, narrow task domains, or unrealistic tool abstractions. We introduce OpenAgentSafety, a comprehensive and modular framework for evaluating agent behavior across eight critical risk categories. Unlike prior work, our framework evaluates agents that interact with real tools, including web browsers, code execution environments, file systems, bash shells, and messaging platforms; and supports over 350 multi-turn, multi-user tasks spanning both benign and adversarial user intents. OpenAgentSafety is designed for extensibility, allowing researchers to add tools, tasks, websites, and adversarial strategies with minimal effort. It combines rule-based analysis with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors. Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7% with o3-mini, highlighting critical safety vulnerabilities and the need for stronger safeguards before real-world deployment.}
}

@inproceedings{columbiaConvening,
title={A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety},
author={Camille François and Ludovic Pérán and Ayah Bdeir and Nouha Dziri and Will Hawkins and Yacine Jernite and Sayash Kapoor and Juliet Shen and Heidy Khlaaf and Kevin Klyman and Nik Marda and Marie Pellat and Deb Raji and Divya Siddarth and Aviya Skowron and Joseph Spisak and Madhulika Srikumar and Victor Storchan and Audrey Tang and Jen Weedon},
booktitle={Arxiv},
month={June},
year={2025},
arxiv={2506.22183},
url={https://arxiv.org/abs/2506.22183},
preview={columbia.jpg},
abstract={The rapid rise of open-weight and open-source foundation models is intensifying the obligation and reshaping the opportunity to make AI systems safe. This paper reports outcomes from the Columbia Convening on AI Openness and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme involving more than forty-five researchers, engineers, and policy leaders from academia, industry, civil society, and government. Using a participatory, solutions-oriented process, the working groups produced (i) a research agenda at the intersection of safety and open source AI; (ii) a mapping of existing and needed technical interventions and open source tools to safely and responsibly deploy open foundation models across the AI development workflow; and (iii) a mapping of the content safety filter ecosystem with a proposed roadmap for future research and development. We find that openness -- understood as transparent weights, interoperable tooling, and public governance -- can enhance safety by enabling independent scrutiny, decentralized mitigation, and culturally plural oversight. However, significant gaps persist: scarce multimodal and multilingual benchmarks, limited defenses against prompt-injection and compositional attacks in agentic systems, and insufficient participatory mechanisms for communities most affected by AI harms. The paper concludes with a roadmap of five priority research directions, emphasizing participatory inputs, future-proof content filters, ecosystem-wide safety infrastructure, rigorous agentic safeguards, and expanded harm taxonomies. These recommendations informed the February 2025 French AI Action Summit and lay groundwork for an open, plural, and accountable AI safety discipline.}
}

@inproceedings{singaporeAISafety,
title={The Singapore Consensus on Global AI Safety Research Priorities},
author={Yoshua Bengio and Tegan Maharaj and Luke Ong and Stuart Russell and Dawn Song and ... and Jeff Clune and Juntao Dai and Agnes Delaborde and Nouha Dziri and Francisco Eiras and Joshua Engels and Jinyu Fan and Adam Gleave and Noah Goodman and ... and Wei Xu and Rongwu Xu and Yi Zeng and HongJiang Zhang and Djordje Žikelić},
booktitle={Arxiv},
month={June},
year={2025},
arxiv={2506.20702},
url={https://arxiv.org/abs/2506.20702},
preview={singapore.png},
abstract={Rapidly improving AI capabilities and autonomy hold significant promise of transformation, but are also driving vigorous debate on how to ensure that AI is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem is therefore essential -- it helps people embrace AI with confidence and gives maximal space for innovation while avoiding backlash.
The "2025 Singapore Conference on AI (SCAI): International Scientific Exchange on AI Safety" aimed to support research in this space by bringing together AI scientists across geographies to identify and synthesise research priorities in AI safety. This resulting report builds on the International AI Safety Report chaired by Yoshua Bengio and backed by 33 governments. By adopting a defence-in-depth model, this report organises AI safety research domains into three types: challenges with creating trustworthy AI systems (Development), challenges with evaluating their risks (Assessment), and challenges with monitoring and intervening after deployment (Control).}
}

@inproceedings{omega,
title={OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization},
author={Yiyou Sun and Shawn Hu and Georgia Zhou and Ken Zheng and Hannaneh Hajishirzi and Nouha Dziri* and Dawn Song*},
booktitle={NeurIPS},
year={2025},
month={},
arxiv={2506.18880},
url={https://arxiv.org/abs/2506.18880},
preview={omega.png},
code={https://github.com/sunblaze-ucb/math_ood},
abstract={Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.}
}

@inproceedings{reasoningLadder,
title={Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?},
author={Yiyou Sun and Georgia Zhou and Hao Wang and Dacheng Li and Nouha Dziri and Dawn Song},
booktitle={Arxiv},
year={2025},
month={April},
arxiv={2504.11741},
url={https://arxiv.org/abs/2504.11741},
preview={reasoningladder.png},
code={https://github.com/sunblaze-ucb/reasoning_ladder},
abstract={Recent supervised fine-tuning (SFT) approaches have significantly improved language models' performance on mathematical reasoning tasks, even when models are trained at a small scale. However, the specific capabilities enhanced through such fine-tuning remain poorly understood. In this paper, we conduct a detailed analysis of model performance on the AIME24 dataset to understand how reasoning capabilities evolve. We discover a ladder-like structure in problem difficulty, categorize questions into four tiers (Easy, Medium, Hard, and Extremely Hard (Exh)), and identify the specific requirements for advancing between tiers. We find that progression from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT (500-1K instances), while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain, with accuracy plateauing at around 65% despite logarithmic scaling. Exh-level questions present a fundamentally different challenge; they require unconventional problem-solving skills that current models uniformly struggle with. Additional findings reveal that carefully curated small-scale datasets offer limited advantage-scaling dataset size proves far more effective. Our analysis provides a clearer roadmap for advancing language model capabilities in mathematical reasoning.}
}

@inproceedings{trustGen,
title={On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective},
author={Yue Huang and Chujie Gao and Siyuan Wu and Haoran Wang and Xiangqi Wang and Yujun Zhou and Yanbo Wang and Jiayi Ye and Jiawen Shi and Qihui Zhang and Yuan Li and Han Bao and Zhaoyi Liu and Tianrui Guan and Dongping Chen and Ruoxi Chen and Kehan Guo and Andy Zou and Bryan Hooi Kuen-Yew and Caiming Xiong and Elias Stengel-Eskin and Hongyang Zhang and Hongzhi Yin and Huan Zhang and Huaxiu Yao and Jaehong Yoon and Jieyu Zhang and Kai Shu and Kaijie Zhu and Ranjay Krishna and Swabha Swayamdipta and Taiwei Shi and Weijia Shi and Xiang Li and Yiwei Li and Yuexing Hao and Zhihao Jia and Zhize Li and Xiuying Chen and Zhengzhong Tu and Xiyang Hu and Tianyi Zhou and Jieyu Zhao and Lichao Sun and Furong Huang and Or Cohen Sasson and Prasanna Sattigeri and Anka Reuel and Max Lamparth and Yue Zhao and Nouha Dziri and Yu Su and Huan Sun and Heng Ji and Chaowei Xiao and Mohit Bansal and Nitesh V. Chawla and Jian Pei and Jianfeng Gao and Michael Backes and Philip S. Yu and Neil Zhenqiang Gong and Pin-Yu Chen and Bo Li and Xiangliang Zhang},
booktitle={Arxiv},
year={2025},
month={February},
arxiv={2502.14296},
url={https://arxiv.org/abs/2502.14296},
preview={trustgen.png},
website={https://trustgen.github.io/},
abstract={Generative Foundation Models (GenFMs) have emerged as transformative tools. However, their widespread adoption raises critical concerns regarding trustworthiness across dimensions. This paper presents a comprehensive framework to address these challenges through three key contributions. First, we systematically review global AI governance laws and policies from governments and regulatory bodies, as well as industry practices and standards. Based on this analysis, we propose a set of guiding principles for GenFMs, developed through extensive multidisciplinary collaboration that integrates technical, ethical, legal, and societal perspectives. Second, we introduce TrustGen, the first dynamic benchmarking platform designed to evaluate trustworthiness across multiple dimensions and model types, including text-to-image, large language, and vision-language models. TrustGen leverages modular components--metadata curation, test case generation, and contextual variation--to enable adaptive and iterative assessments, overcoming the limitations of static evaluation methods. Using TrustGen, we reveal significant progress in trustworthiness while identifying persistent challenges. Finally, we provide an in-depth discussion of the challenges and future directions for trustworthy GenFMs, which reveals the complex, evolving nature of trustworthiness, highlighting the nuanced trade-offs between utility and trustworthiness, and consideration for various downstream applications, identifying persistent challenges and providing a strategic roadmap for future research. This work establishes a holistic framework for advancing trustworthiness in GenAI, paving the way for safer and more responsible integration of GenFMs into critical applications. To facilitate advancement in the community, we release the toolkit for dynamic evaluation.}
}

@inproceedings{olmo2,
title={2 OLMo 2 Furious},
author={Evan Pete Walsh and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Shane Arora and Akshita Bhagia and Yuling Gu and Shengyi Huang and Matt Jordan and Nathan Lambert and Dustin Schwenk and Oyvind Tafjord and Taira Anderson and David Atkinson and Faeze Brahman and Christopher Clark and Pradeep Dasigi and Nouha Dziri and Allyson Ettinger and Michal Guerquin and David Heineman and Hamish Ivison and Pang Wei Koh and Jiacheng Liu and Saumya Malik and William Merrill and Lester James Validad Miranda and Jacob Morrison and Tyler Murray and Crystal Nam and Jake Poznanski and Valentina Pyatkin and Aman Rangapur and Michael Schmitz and Sam Skjonsberg and David Wadden and Christopher Wilhelm and Michael Wilson and Luke Zettlemoyer and Ali Farhadi and Noah A. Smith and Hannaneh Hajishirzi},
booktitle={COLM},
month={},
year={2025},
arxiv={2501.00656},
url={https://openreview.net/forum?id=2ezugTT9kU},
preview={olmo2.png},
code={https://github.com/allenai/OLMo-core},
abstract={We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes a family of dense autoregressive language models at 7B, 13B and 32B scales with fully released artifacts -- model weights, full training data, training code and recipes, training logs and thousands of intermediate checkpoints. In this work, we describe our modified model architecture and training recipe, focusing on techniques for achieving better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from Tülu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to training compute, often matching or outperforming open-weight only models like Llama 3.1, Qwen 2.5, and Gemma 2 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with open-weight only models of comparable size and even some proprietary models like GPT-3.5 Turbo and GPT 4o Mini.}
}


@inproceedings{tulu3,
title={Tulu 3: Pushing Frontiers in Open Language Model Post-Training},
author={Nathan Lambert and Jacob Morrison and Valentina Pyatkin and Shengyi Huang and Hamish Ivison and Faeze Brahman and Lester James Validad Miranda and Alisa Liu and Nouha Dziri and Xinxi Lyu and Yuling Gu and Saumya Malik and Victoria Graf and Jena D. Hwang and Jiangjiang Yang and Ronan Le Bras and Oyvind Tafjord and Christopher Wilhelm and Luca Soldaini and Noah A. Smith and Yizhong Wang and Pradeep Dasigi and Hannaneh Hajishirzi},
booktitle={COLM},
month={},
year={2025},
arxiv={2411.15124},
url={https://openreview.net/forum?id=i1uGbfHHpH},
preview={tulu3.png},
code={https://github.com/allenai/open-instruct},
abstract={Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce Tulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. Tulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance. In addition to the Tulu 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the Tulu 3 approach to more domains.}
}

@inproceedings{safetyanalyst,
title={SafetyAnalyst: Interpretable, Transparent, and Steerable Safety Moderation for AI Behavior},
author={Jing-Jing Li and Valentina Pyatkin and Max Kleiman-Weiner and Liwei Jiang and Nouha Dziri and Anne GE Collins and Jana Schaich Borg and Maarten Sap and Yejin Choi and Sydney Levine},
booktitle={ICML},
month={},
year={2025},
arxiv={2410.16665},
preview={safetyanalyst.png},
abstract={The ideal AI safety moderation system would be both structurally interpretable (so its decisions can be reliably explained) and steerable (to align to safety standards and reflect a community's values), which current systems fall short on. To address this gap, we present SafetyAnalyst, a novel AI safety moderation framework. Given an AI behavior, SafetyAnalyst uses chain-of-thought reasoning to analyze its potential consequences by creating a structured "harm-benefit tree," which enumerates harmful and beneficial actions and effects the AI behavior may lead to, along with likelihood, severity, and immediacy labels that describe potential impacts on stakeholders. SafetyAnalyst then aggregates all effects into a harmfulness score using 28 fully interpretable weight parameters, which can be aligned to particular safety preferences. We applied this framework to develop an open-source LLM prompt safety classification system, distilled from 18.5 million harm-benefit features generated by frontier LLMs on 19k prompts. On comprehensive benchmarks, we show that SafetyAnalyst (average F1=0.81) outperforms existing moderation systems (average F1 0.72) on prompt safety classification, while offering the additional advantages of interpretability, transparency, and steerability.}
}


@inproceedings{olmoSafety,
title={To Err Is AI: A Case Study Informing LLM Flaw Reporting Practices},
author={Sean McGregor and Allyson Ettinger and Nick Judd and Paul Albee and Liwei Jiang and Kavel Rao and William H. Smith and Shayne Longpre and Avijit Ghosh and Christopher Fiorelli and Michelle Hoang and Sven Cattell and Nouha Dziri},
booktitle={AAAI},
month={},
year={2025},
arxiv={2410.12104},
url={https://doi.org/10.1609/aaai.v39i28.35162},
preview={olmosafety.png},
abstract={In August of 2024, 495 hackers generated evaluations in an open-ended bug bounty targeting the Open Language Model (OLMo) from The Allen Institute for AI. A vendor panel staffed by representatives of OLMo's safety program adjudicated changes to OLMo's documentation and awarded cash bounties to participants who successfully demonstrated a need for public disclosure clarifying the intent, capacities, and hazards of model deployment. This paper presents a collection of lessons learned, illustrative of flaw reporting best practices intended to reduce the likelihood of incidents and produce safer large language models (LLMs). These include best practices for safety reporting processes, their artifacts, and safety program staffing.}
}

@inproceedings{creativity,
title={AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text},
author={Ximing Lu and Melanie Sclar and Skyler Hallinan and Niloofar Mireshghallah and Jiacheng Liu and Seungju Han and Allyson Ettinger and Liwei Jiang and Khyathi Chandu and Nouha Dziri and Yejin Choi},
booktitle={ICLR (Oral)},
month={},
year={2025},
arxiv={2410.04265},
url={https://openreview.net/forum?id=ilOEOIqolQ},
preview={creativity_index.png},
code={https://github.com/GXimingLu/creativity_index},
abstract={Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpass human creativity. We present CREATIVITY INDEX as the first step to quantify the linguistic creativity of a text by reconstructing it from existing text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that the seemingly remarkable creativity of LLMs may be attributable in large part to the creativity of human-written texts on the web. To compute CREATIVITY INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. Experiments reveal that the CREATIVITY INDEX of professional human authors is on average 66.2% higher than that of LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of 30.1%. In addition, we find that distinguished authors like Hemingway exhibit measurably higher CREATIVITY INDEX compared to other human writers. Finally, we demonstrate that CREATIVITY INDEX can be used as a surprisingly effective criterion for zero-shot machine text detection, surpassing the strongest existing zero-shot system, DetectGPT, by a significant margin of 30.2%, and even outperforming the strongest supervised system, GhostBuster, in five out of six domains.}
}

@inproceedings{relai,
  title={Rel-AI: An Interaction-Centered Approach To Measuring Human-LM Reliance},
  author={Kaitlyn Zhou and Jena D Hwang and Xiang Ren and Nouha Dziri and Dan Jurafsky and Maarten Sap},
  booktitle={NAACL (Best Paper Runner Up)},
  month={},
  year={2025},
  url={https://aclanthology.org/2025.naacl-long.556/},
  preview={rel.png},
  arxiv={2407.07950},
  abstract={The ability to communicate uncertainty and knowledge limitations is crucial for the safety of large language models (LLMs). Current evaluations of these abilities typically examine the correspondence between model accuracy and its internal probabilities or linguistic outputs. However, evaluation of the uncertainty of LLM communication should also focus on the behaviors of their human interlocutors: how much do users rely on what the LLM says? We introduce an interaction-centered evaluation approach called Rel-A.I. (pronounced “rely”) that quantifies whether and how humans rely on LLMs’ responses, complementing existing calibration evaluations. Through nine user studies with 450 participants, we investigate three crucial aspects that influence user reliance. We show that emphatic expressions of politeness (e.g., “I’m happy to help!”) that precede LLM answers will cause participants to perceive these models as more competent, and in turn, rely 30% more on their generations. Additionally, the context of the interaction, such as the knowledge domain and nature of previous interactions with the LLM, substantially influences user reliance (e.g., users will rely 10% more on LLMs when responding to questions involving calculations). Our results show that calibration and language quality alone are insufficient in informing which LLMs are safely calibrated, and illustrate the need to consider features of the interactional context.}
}


@inproceedings{Wildguard,
  title={WildGuard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of LLMs},
  author={Seungju Han and Kavel Rao and Allyson Ettinger and Liwei Jiang and Bill Yuchen Lin and Nathan Lambert and Yejin Choi and Nouha Dziri},
  booktitle={NeurIPS},
  month={},
  year={2024},
  url={https://arxiv.org/abs/2406.18510},
  preview={wildguard.png},
  code={https://github.com/allenai/wildguard},
  arxiv={2406.18495},
abstract={We introduce WildGuard -- an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WildGuard serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks and in evaluating models' refusals, a key measure for evaluating safety behaviors in model responses.
}}


@inproceedings{WildTeaming,
  title={WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models},
  author={Seungju Han and Kavel Rao and Allyson Ettinger and Liwei Jiang and Bill Yuchen Lin and Nathan Lambert and Yejin Choi and Nouha Dziri},
  booktitle={NeurIPS},
  month={},
  year={2024},
  url={https://arxiv.org/abs/2406.18495},
  preview={wildteaming.png},
  code={https://github.com/allenai/wildteaming},
  arxiv={2406.04770},
abstract={We introduce WildGuard -- an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WildGuard serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks and in evaluating models' refusals, a key measure for evaluating safety behaviors in model responses. To address these challenges, we construct WildGuardMix, a large-scale and carefully balanced multi-task safety moderation dataset with 92K labeled examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired with various refusal and compliance responses. WildGuardMix is a combination of WildGuardTrain, the training data of WildGuard, and WildGuardTest, a high-quality human-annotated moderation test set with 5K labeled items covering broad risk scenarios. Through extensive evaluations on WildGuardTest and ten existing public benchmarks, we show that WildGuard establishes state-of-the-art performance in open-source safety moderation across all the three tasks compared to ten strong existing open-source moderation models (e.g., up to 26.4 improvement on refusal detection).
Importantly, WildGuard matches and sometimes exceeds GPT-4.}}


@inproceedings{wildBench,
  title={WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild},
  author={Bill Yuchen Lin and Yuntian Deng and Khyathi Chandu and Faeze Brahman and Abhilasha Ravichander and Valentina Pyatkin and Nouha Dziri and Ronan Le Bras and Yejin Choi},
  booktitle={ICLR},
  month={},
  year={2025},
  url={https://arxiv.org/abs/2406.04770},
  preview={wildbench.png},
  code={https://github.com/allenai/WildBench},
  website={https://huggingface.co/spaces/allenai/WildBench},
  arxiv={2406.04770},
abstract={We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries.
WILDBENCH consists of 1,024 tasks carefully selected from over one million
human-chatbot conversation logs. For automated evaluation with WILDBENCH,
we have developed two metrics, WB-Reward and WB-Score, which are computable
using advanced LLMs such as GPT-4-turbo. WILDBENCH evaluation uses taskspecific checklists to evaluate model outputs systematically and provides structured
explanations that justify the scores and comparisons, resulting in more reliable
and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes:
much better, slightly better, slightly worse, much worse, or a tie. Unlike previous
evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation.
Additionally, we propose a simple method to mitigate length bias, by converting
outcomes of “slightly better/worse” to “tie” if the winner response exceeds the
loser one by more than K characters. WB-Score evaluates the quality of model
outputs individually, making it a fast and cost-efficient evaluation metric. WILDBENCH results demonstrate a strong correlation with the human-voted Elo ratings
from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson
correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95,
surpassing both ArenaHard’s 0.91 and AlpacaEval2.0’s 0.89 for length-controlled
win rates, as well as the 0.87 for regular win rates.}}



@inproceedings{rewardBench,
  title={RewardBench: Evaluating reward models for language modeling},
  author={Nathan Lambert and Valentina Pyatkin and Jacob Morrison and LJ Miranda and Bill Yuchen Lin and Khyathi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A Smith and Hannaneh Hajishirzi},
  booktitle={Arxiv},
  month=March,
  year={2024},
  url={https://arxiv.org/abs/2403.13787},
  preview={reward.png},
  code={https://github.com/allenai/reward-bench},
  arxiv={2403.13787},
abstract={Reward models (RMs) are at the crux of successfully using RLHF to align pretrained models to human preferences, yet there has been relatively little study
that focuses on evaluation of those models. Evaluating reward models presents
an opportunity to understand thereward
 opaque technologies used for alignment of language models and which values are embedded in them. Resources for reward
model training and understanding are sparse in the nascent open-source community around them. To enhance scientific understanding of reward models, we
present REWARDBENCH, a benchmark dataset and code-base for evaluation. The
REWARDBENCH dataset is a collection of prompt-chosen-rejected trios spanning
chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We create specific comparison
datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts)
why one answer should be preferred to another. On the REWARDBENCH leaderboard, we evaluate reward models trained with a variety of methods, such as the
direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO). We present many findings on propensity for refusals,
reasoning limitations, and instruction following shortcomings of various reward
models towards a better understanding of the RLHF process.}}



@inproceedings{pluralistic,
  title={A roadmap to pluralistic alignment},
  author={Taylor Sorensen and Jared Moore and Jillian Fisher and Mitchell Gordon and Niloofar Mireshghallah and Christopher Michael Rytting and Andre Ye and Liwei Jiang and Ximing Lu and Nouha Dziri and Tim Althoff and Yejin Choi},
  booktitle={ICML},
  month={},
  year={2024},
  url={https://arxiv.org/abs/2402.05070},
  preview={plua.png},
  arxiv={2402.05070},
abstract={With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses;
 2) Steerably pluralistic models that can steer to reflect certain perspectives; and
  3) Distributionally pluralistic models that are well-calibrated to a given population in
  distribution. We also propose and formalize three possible classes of pluralistic benchmarks:
  1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs,
  and 3) Jury-pluralistic benchmarks which explicitly model diverse human ratings. We use this framework to argue that current alignment techniques may
  be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard
  alignment procedures might reduce distributional pluralism in models, motivating the need for further research on pluralistic alignment.}}

@inproceedings{inductive,
  title={Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement},
  author={Linlu Qiu and Liwei Jiang and Ximing Lu and Melanie Sclar and  Valentina Pyatkin and Chandra Bhagavatula and Bailin Wang and Yoon Kim and Yejin Choi and Nouha Dziri and Xiang Ren},
  booktitle={ICLR (Oral)},
  month={},
  year={2024},
  url={https://arxiv.org/abs/2310.08559},
  preview={inductive.png},
  code={https://github.com/linlu-qiu/lm-inductive-reasoning},
  arxiv={2310.08559},
abstract={The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.}
}

@inproceedings{paradox,
  title={The Generative AI Paradox: What It Can Create, It May Not Understand},
  author={Peter West* and Ximing Lu* and Nouha Dziri* and Faeze Brahman* and Linjie Li and Jena D Hwang and Liwei Jiang and Jillian Fisher and Abhilasha Ravichander and Khyathi Chandu and Benjamin Newman and Pang Wei Koh and Allyson Ettinger and Yejin Choi},
  booktitle={ICLR},
  month={},
  year={2024},
  url={https://arxiv.org/abs/2311.00059},
  preview={paradox.png},
  arxiv={2311.00059},
abstract={The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. We test this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities. Our results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models' generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.
}
}

@inproceedings{urial,
  title={The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning},
  author={Bill Yuchen Lin  and Abhilasha Ravichander  and Ximing Lu and Nouha Dziri  and Melanie Sclar and Khyathi Chandu and Chandra Bhagavatula and Yejin Choi},
  booktitle={ICLR},
  month={},
  year={2024},
  url={https://arxiv.org/abs/2312.01552},
  preview={urial.png},
  arxiv={2312.01552},
  code={https://github.com/Re-Align/just-eval},
  website={https://allenai.github.io/re-align/},
  data={https://github.com/Re-Align/urial},
abstract={The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be "superficial." This raises questions about how exactly the alignment tuning transforms a base LLM.
We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA.
Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.}
}



@inproceedings{Kaleidoscope,
  title={Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties},
  author={Taylor Sorensen  and Liwei Jiang  and Jena Hwang and Sydney Levine  and Valentina Pyatkin  and Peter West and Nouha Dziri  and Ximing Lu  and Kavel Rao  and Chandra Bhagavatula  and Maarten Sap  and John Tasioulas  and Yejin Choi},
  booktitle={AAAI},
  month=Sept,
  year={2024},
  url={https://arxiv.org/abs/2309.00779},
  preview={kaleido.png},
  arxiv={2309.00779},
  code={https://github.com/tsor13/kaleido},
  website={https://kaleido.allen.ai/},
  data={https://huggingface.co/datasets/allenai/ValuePrism},
abstract={Visual information is central to conversation: body gestures and facial expressions, for example, contribute to meaning that transcends words alone. To date, however, most neural conversational models are limited to just text. We introduce CHAMPAGNE, a generative model of conversations that can account for visual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that converts error-prone automatic transcripts to a cleaner dialogue format while maintaining meaning. Human evaluation reveals that YTD-18M is more sensible and specific than prior resources (MMDialog, 1M dialogues), while maintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE learns to conduct conversation from YTD-18M; and 2) when fine-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations. We release data, models, and code at https://seungjuhan.me/champagne.}
}

@inproceedings{faith,
  title={Faith and Fate: Limits of Transformers on Compositionality},
  author={Nouha {Dziri} and
                  Ximing {Lu} and
                  Melanie {Sclar} and
                  Xiang Lorraine {Li} and
                  Liwei {Jiang} and
                  Bill Yuchen {Lin} and
                  Peter {West} and
                  Chandra {Bhagavatula} and
                  Ronan {Le Bras} and
                  Jena D. {Hwang} and
                  Soumya {Sanyal} and
                  Sean {Welleck} and
                  Xiang {Ren} and
                  Allyson {Ettinger} and
                  Zaid {Harchaoui} and
                  Yejin {Choi}},
  booktitle={NeurIPS (Spotlight)},
  month=June,
  year={2023},
  url={https://arxiv.org/abs/2305.18654},
  preview={faith.png},
  arxiv={2305.18654},
  abstract={Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how Transformers' performance will rapidly decay with increased task complexity.}
  }




@inproceedings{rlhf,
  title={Fine-Grained Human Feedback Gives Better Rewards for Language Model Training},
  author={Zeqiu Wu  and Yushi Hu  and Weijia Shi  and Nouha Dziri  and Alane Suhr  and Prithviraj Ammanabrolu  and Noah Smith  and Mari Ostendorf  and Hannaneh Hajishirzi},
  booktitle={NeurIPS (Spotlight)},
  month=June,
  year={2023},
  url={https://arxiv.org/abs/2306.01693},
  preview={rlhf.png},
  arxiv={2306.01693},
  code={https://github.com/allenai/FineGrainedRLHF},
    website={https://finegrainedrlhf.github.io/},
abstract={Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.}
}

@inproceedings{ethic,
  title={What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations},
  author={Kavel Rao, Liwei Jiang, Valentina Pyatkin, Yuling Gu, Faeze Brahman, Niket Tandon, Nouha Dziri, Faeze Brahman, Yejin Choi},
  booktitle={EMNLP},
  month=May,
  year={2023},
  url={https://aclanthology.org/2023.findings-emnlp.812/},
  preview={defeasible.png},
  arxiv={2310.15431},
  abstract={Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and intricacy of grounded human moral judgment in real-life scenarios.
We introduce defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning. To elicit high-quality task data, we take an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between (1) self-distillation from student models; (2) targeted filtering with a critic model trained by human judgment (to boost validity) and NLI (to boost diversity); (3) self-imitation learning (to amplify the desired data quality). This process yields a student model that produces defeasible contexts with improved validity, diversity, and defeasibility. From this model we distill a high-quality dataset, \delta-Rules-of-Thumb, of 1.2M entries of contextualizations and rationales for 115K defeasible moral actions rated highly by human annotators 85.9% to 99.8% of the time. Using \delta-RoT we obtain a final student model that wins over all intermediate student models by a notable margin.}
}

@inproceedings{ipa,
  title={Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning},
  author={Ximing Lu and Faeze Brahman and Peter West and Jaehun Jang and Khyathi Chandu and Abhilasha Ravichander and Lianhui Qin and Prithviraj Ammanabrolu and Liwei Jiang and Sahana Ramnath and Nouha Dziri and Jillian Fisher and Bill Yuchen Lin and Skyler Hallinan and Xiang Ren and Sean Welleck and Yejin Choi},
  booktitle={EMNLP},
  month=May,
  year={2023},
  url={https://arxiv.org/pdf/2305.15065},
  preview={ipa.png},
  arxiv={2305.15065},
abstract={Large language models excel at a variety of language tasks when prompted with examples or instructions. Yet controlling these models through prompting alone is limited. Tailoring language models through fine-tuning (e.g., via reinforcement learning) can be effective, but it is expensive and requires model access. We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adaptor trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and open-domain generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT- 3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.}
}

@inproceedings{refine,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Aman Madaan and
                  Niket Tandon and
                  Prakhar Gupta and
                  Skyler Hallinan and
                  Luyu Gao and
                  Sarah Wiegreffe and
                  Uri Alon and
                  Nouha Dziri and
                  Shrimai Prabhumoye and
                  Yiming Yang and
                  Sean Welleck and
                  Bodhisattwa Prasad Majumder and
                  Shashank Gupta and
                  Amir Yazdanbakhsh and
                  Peter Clark},
  booktitle={NeurIPS},
  month=Mar,
  year={2023},
  url={https://arxiv.org/abs/2303.17651},
  arxiv={2303.17651},
  preview={refine.png},
  website={https://selfrefine.info/},
  code={https://github.com/madaan/self-refine},
abstract={Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.}}

@inproceedings{CHAMPAGNE,
  title={CHAMPAGNE: Learning Real-world Conversation
from Large-Scale Web Videos},
  author={Seungju Han  and Jack Hessel  and Nouha Dziri  and Yejin Choi  and Youngjae Yu},
  booktitle={ICCV},
  month=March,
  year={2023},
  url={https://arxiv.org/pdf/2303.09713},
  website={https://seungjuhan.me/champagne/},
  code={https://github.com/wade3han/champagne},
  preview={champagne.png},
  arxiv={2303.09713},
abstract={Visual information is central to conversation: body gestures and facial expressions, for example, contribute to meaning that transcends words alone. To date, however, most neural conversational models are limited to just text. We introduce CHAMPAGNE, a generative model of conversations that can account for visual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that converts error-prone automatic transcripts to a cleaner dialogue format while maintaining meaning. Human evaluation reveals that YTD-18M is more sensible and specific than prior resources (MMDialog, 1M dialogues), while maintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE learns to conduct conversation from YTD-18M; and 2) when fine-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations. We release data, models, and code at https://seungjuhan.me/champagne.}
}


@inproceedings{elastic,
  title={Elastic weight removal for faithful and abstractive dialogue generation},
  author={Nico Daheim  and Nouha Dziri  and Mrinmaya Sachan  and Iryna Gurevych  and Edoardo M Ponti},
  month={},
  year={2024},
  booktitle={NAACL},
  code={https://github.com/ndaheim/faithful-dialogue},
  arxiv={2303.17574},
  url={aclanthology.org/2024.naacl-long.393/},
  preview={ewr.png},
abstract={Ideally, dialogue systems should generate responses that are faithful to the knowledge contained in relevant documents. However, many models generate hallucinated responses instead that contradict it or contain unverifiable information. To mitigate such undesirable behaviour, it has been proposed to fine-tune a `negative expert' on negative examples and subtract its parameters from those of a pre-trained model. However, intuitively, this does not take into account that some parameters are more responsible than others in causing hallucinations. Thus, we propose to weigh their individual importance via (an approximation of) the Fisher Information matrix, which measures the uncertainty of their estimate. We call this method Elastic Weight Removal (EWR). We evaluate our method -- using different variants of Flan-T5 as a backbone language model -- on multiple datasets for information-seeking dialogue generation and compare our method with state-of-the-art techniques for faithfulness, such as CTRL, Quark, DExperts, and Noisy Channel reranking. Extensive automatic and human evaluation shows that EWR systematically increases faithfulness at minor costs in terms of other metrics. However, we notice that only discouraging hallucinations may increase extractiveness, i.e. shallow copy-pasting of document spans, which can be undesirable. Hence, as a second main contribution, we show that our method can be extended to simultaneously discourage hallucinations and extractive responses. We publicly release the code for reproducing EWR and all baselines.}}


@inproceedings{evalOpenQA,
  title={Evaluating Open-Domain Question Answering in the Era of Large Language Models},
  author={{Kamalloo}, Ehsan and {Dziri}, Nouha and {Clarke}, Charles and {Rafiei}, Davood},
  month = jul,
  year = {2023},
  code={https://github.com/ehsk/OpenQA-eval},
  url={https://aclanthology.org/2023.acl-long.307/},
  arxiv={2305.06984},
  preview={qa.png},
  booktitle = {ACL (oral)},
  location = {Toronto, Canada},
  abstract={Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-open. We also find that more than 50% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.},
}



@article{begin,
  title={Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark},
  author={Nouha Dziri and Hannah Rashkin and Tal Linzen and David Reitter},
  journal={TACL},
  month=May,
  year={2022},
  pdf={https://aclanthology.org/2022.tacl-1.62.pdf},
  code={https://github.com/google/BEGIN-dataset},
  preview={begin.png},
  abstract={The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. Dziri et al. (2022)'s investigation of hallucinations has revealed that existing knowledge-grounded benchmarks are contaminated with hallucinated responses at an alarming level (>60% of the responses) and models trained on this data amplify hallucinations even further (>80% of the responses). To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as a training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 21.1 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging.},
}


@article{faithdial,
  title={FaithDial: A Faithful Benchmark for Information-Seeking Dialogue},
  author={{Dziri}, Nouha and {Kamalloo}, Ehsan and {Milton}, Sivan and Zaiane, Osmar and Yu, Mo and Ponti, Edoardo and Reddy, Siva},
  journal={TACL},
  month=apr,
  year={2022},
  url={https://arxiv.org/abs/2204.10757},
  arxiv={2204.10757},
  website={https://mcgill-nlp.github.io/FaithDial/},
  code={https://github.com/McGill-NLP/FaithDial},
  preview={faithdial.png},
  abstract={The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. Dziri et al. (2022)'s investigation of hallucinations has revealed that existing knowledge-grounded benchmarks are contaminated with hallucinated responses at an alarming level (>60% of the responses) and models trained on this data amplify hallucinations even further (>80% of the responses). To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as a training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 21.1 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging.},
}


@article{hall,
  title={On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?},
  author={Nouha Dziri and Sivan Milton and Mo Yu and Osmar Zaiane and Siva Reddy},
  journal={NAACL},
  month=Jul,
  year={2022},
  url={https://arxiv.org/pdf/2204.07931},
  arxiv={2204.07931},
  code={https://github.com/McGill-NLP/FaithDial},
  preview={hall.png},
abstract={Knowledge-grounded conversational models are known to suffer from producing factually invalid statements, a phenomenon commonly called hallucination. In this work, we investigate the underlying causes of this phenomenon: is hallucination due to the training data, or to the models? We conduct a comprehensive human study on both existing knowledge-grounded conversational benchmarks and several state-of-the-art models. Our study reveals that the standard benchmarks consist of >60% hallucinated responses, leading to models that not only hallucinate but even amplify hallucinations. Our findings raise important questions on the quality of existing datasets and models trained using them. We make our annotations publicly available for future research.}
}


@article{nph,
  title={Neural path hunter: Reducing hallucination in dialogue systems via path grounding},
  author={Nouha Dziri and Andrea Madotto and Osmar Zaiane and Avishek Joey Bose},
  journal={EMNLP},
  month=Nov,
  year={2021},
  url={https://arxiv.org/pdf/2104.08455},
  arxiv={2104.08455},
  code={https://github.com/nouhadziri/Neural-Path-Hunter},
  preview={nph.png},
abstract={Dialogue systems powered by large pre-trained language models (LM) exhibit an innate ability to deliver fluent and natural-looking responses. Despite their impressive generation performance, these models can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving the faithfulness -- and thus reduce hallucination -- of Neural Dialogue Systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the k-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage consisting of a chain of two neural LM's that retrieves correct entities by crafting a query signal that is propagated over the k-hop subgraph. Our proposed model can easily be applied to any dialogue generated responses without retraining the model. We empirically validate our proposed approach on the OpenDialKG dataset against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020).}}

@article{demi,
  title={Decomposed mutual information estimation for contrastive representation learning},
  author={Alessandro Sordoni* and Nouha Dziri* and Hannes Schulz* and Geoff Gordon and Philip Bachman and Remi Tachet Des Combes},
  journal={ICML},
  month=Jul,
  year={2021},
  pdf={http://proceedings.mlr.press/v139/sordoni21a/sordoni21a.pdf},
  preview={demi.png},
abstract={Recent contrastive representation learning methods rely on estimating mutual information (MI) between multiple views of an underlying context. Eg, we can derive multiple views of a given image by applying data augmentation, or we can split a sequence into views comprising the past and future of some step in the sequence. Contrastive lower bounds on MI are easy to optimize, but have a strong underestimation bias when estimating large amounts of MI. We propose decomposing the full MI estimation problem into a sum of smaller estimation problems by splitting one of the views into progressively more informed subviews and by applying the chain rule on MI between the decomposed views. This expression contains a sum of unconditional and conditional MI terms, each measuring modest chunks of the total MI, which facilitates approximation via contrastive bounds. To maximize the sum, we formulate a contrastive lower bound on the conditional MI which can be approximated efficiently. We refer to our general approach as Decomposed Estimation of Mutual Information (DEMI). We show that DEMI can capture a larger amount of MI than standard non-decomposed contrastive bounds in a synthetic setting, and learns better representations in a vision domain and for dialogue generation.}
}




@inproceedings{dziri-etal-2019-evaluating,
    title = "Evaluating Coherence in Dialogue Systems using Entailment",
    author = "Dziri, Nouha  and
      Kamalloo, Ehsan  and
      Mathewson, Kory  and
      Zaiane, Osmar",
    booktitle = "NAACL",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    url = "https://aclanthology.org/N19-1381",
    doi = "10.18653/v1/N19-1381",
    pages = "3806--3812",
    preview={eval.png},
    abstract = "Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.",
    code={https://github.com/nouhadziri/DialogEntailment},
}


@inproceedings{THRED,
    title = "Augmenting Neural Response Generation with Context-Aware Topical Attention",
    author = "Dziri, Nouha  and
      Kamalloo, Ehsan  and
      Mathewson, Kory  and
      Zaiane, Osmar",
    booktitle = "Proceedings of the First Workshop on NLP for Conversational AI (NLP4ConvAI) at ACL 2019",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4103",
    doi = "10.18653/v1/W19-4103",
    pages = "18--31",
    preview={thred.png},
    abstract = "Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges. Notwithstanding the syntactically well-formed responses generated by these neural network models, they are prone to be acontextual, short and generic. In this work, we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven, multi-turn response generation system intended to produce contextual and topic-aware responses. Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation. To train our model, we provide a clean and high-quality conversational dataset mined from Reddit comments. We evaluate THRED on two novel automated metrics, dubbed Semantic Similarity and Response Echo Index, as well as with human evaluation. Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines.",
    code={https://github.com/nouhadziri/THRED},
    preview={THRED.png},

}




@inproceedings{huang-etal-2018-automatic,
    title = "Automatic Dialogue Generation with Expressed Emotions",
    author = {Huang, Chenyang  and
      Za{\"\i}ane, Osmar  and
      Trabelsi, Amine  and
      Dziri, Nouha},
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2008",
    code ={https://github.com/chenyangh/DialogueGenerationWithEmotion},
    doi = "10.18653/v1/N18-2008",
    preview={emotion.png},
    pages = "49--54",
    abstract = "Despite myriad efforts in the literature designing neural dialogue generation systems in recent years, very few consider putting restrictions on the response itself. They learn from collections of past responses and generate one based on a given utterance without considering, speech act, desired style or emotion to be expressed. In this research, we address the problem of forcing the dialogue generation to express emotion. We present three models that either concatenate the desired emotion with the source input during the learning, or push the emotion in the decoder. The results, evaluated with an emotion tagger, are encouraging with all three models, but present better outcome and promise with our model that adds the emotion vector in the decoder.",
}

