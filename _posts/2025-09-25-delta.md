---
layout: post
title: RL Grokking Recipe -- How Can We Enable LLMs to Solve Previously Unsolvable Tasks with RL?
date: 2025-09-25 12:01:01
description: Recent analyses say RL stays on a leash pass@1 goes up, but what’s possible at large sampling (e.g., pass@128) doesn’t expand. We set out to test this directly, and our answer is RL can discover something new, but only when trained wisely.
redirect: https://accessible-dragon-75f.notion.site/RL-Grokking-Recipe-How-Can-We-Enable-LLMs-to-Solve-Previously-Unsolvable-Tasks-with-RL-100a1714e6778062bae5eafad8e7677d
thumbnail: assets/img/grok.png
read_time: 5
---

OpenAI set the AI world abuzz with the release of their o1 models. As the dust settles on this news,  I can't help but feel this is the perfect moment to share my thoughts on LLMs reasoning as someone who's spent a good chunk of my research on understanding the capabilities of LLMs on compositional reasoning tasks...