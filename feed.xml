<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://nouhadziri.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nouhadziri.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-14T12:25:31+00:00</updated><id>https://nouhadziri.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">RL Grokking Recipe – How Can We Enable LLMs to Solve Previously Unsolvable Tasks with RL?</title><link href="https://nouhadziri.github.io/blog/2025/delta/" rel="alternate" type="text/html" title="RL Grokking Recipe – How Can We Enable LLMs to Solve Previously Unsolvable Tasks with RL?"/><published>2025-09-25T12:01:01+00:00</published><updated>2025-09-25T12:01:01+00:00</updated><id>https://nouhadziri.github.io/blog/2025/delta</id><content type="html" xml:base="https://nouhadziri.github.io/blog/2025/delta/"><![CDATA[<p>OpenAI set the AI world abuzz with the release of their o1 models. As the dust settles on this news, I can’t help but feel this is the perfect moment to share my thoughts on LLMs reasoning as someone who’s spent a good chunk of my research on understanding the capabilities of LLMs on compositional reasoning tasks…</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Can RL actually teach large language models new algorithms—or does it only “sharpen” what’s already latent in the base model?]]></summary></entry><entry><title type="html">Can LLMs Reason Outside the Box in Math?</title><link href="https://nouhadziri.github.io/blog/2025/omega/" rel="alternate" type="text/html" title="Can LLMs Reason Outside the Box in Math?"/><published>2025-06-24T12:01:01+00:00</published><updated>2025-06-24T12:01:01+00:00</updated><id>https://nouhadziri.github.io/blog/2025/omega</id><content type="html" xml:base="https://nouhadziri.github.io/blog/2025/omega/"><![CDATA[<p>Large language models (LLMs) like GPT-4, Claude, and DeepSeek-R1 have made headlines for their impressive performance on ….</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization]]></summary></entry><entry><title type="html">DeepSeek R1: Innovative Research and Engineering Can Rival Brute-Force Scaling</title><link href="https://nouhadziri.github.io/blog/2025/r1/" rel="alternate" type="text/html" title="DeepSeek R1: Innovative Research and Engineering Can Rival Brute-Force Scaling"/><published>2025-01-30T12:01:01+00:00</published><updated>2025-01-30T12:01:01+00:00</updated><id>https://nouhadziri.github.io/blog/2025/r1</id><content type="html" xml:base="https://nouhadziri.github.io/blog/2025/r1/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Nice display of engineering and research]]></summary></entry><entry><title type="html">Current Paradigms of LLMs Safety Alignment are superficial</title><link href="https://nouhadziri.github.io/blog/2024/superficial-safety/" rel="alternate" type="text/html" title="Current Paradigms of LLMs Safety Alignment are superficial"/><published>2024-09-30T12:01:01+00:00</published><updated>2024-09-30T12:01:01+00:00</updated><id>https://nouhadziri.github.io/blog/2024/superficial-safety</id><content type="html" xml:base="https://nouhadziri.github.io/blog/2024/superficial-safety/"><![CDATA[<p>It took me a while to start writing this blog post because honestly my feelings about LLMs safety were pretty complicated…</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Discover why LLMs Safety Alignment methods are Superficial?]]></summary></entry><entry><title type="html">Have o1 Models Cracked Human Reasoning?</title><link href="https://nouhadziri.github.io/blog/2024/o1/" rel="alternate" type="text/html" title="Have o1 Models Cracked Human Reasoning?"/><published>2024-09-23T12:01:01+00:00</published><updated>2024-09-23T12:01:01+00:00</updated><id>https://nouhadziri.github.io/blog/2024/o1</id><content type="html" xml:base="https://nouhadziri.github.io/blog/2024/o1/"><![CDATA[<p>OpenAI set the AI world abuzz with the release of their o1 models. As the dust settles on this news, I can’t help but feel this is the perfect moment to share my thoughts on LLMs reasoning as someone who’s spent a good chunk of my research on understanding the capabilities of LLMs on compositional reasoning tasks…</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Discover how o1 models work in a speculative exploration, and discover whether LLMs have cracked human reasoning.]]></summary></entry></feed>